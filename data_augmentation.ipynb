{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581ef16-ee8a-4b16-a706-b22b4a73a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the TSV file into a DataFrame\n",
    "file_path = 'constraint-corrections-oneOf.tsv'\n",
    "\n",
    "# Assign column names\n",
    "col_names = [\n",
    "    \"constraint.statement\",\n",
    "    \"revision.id.url\",\n",
    "    \"subject.t0\", \"predicate.t0\", \"object.t0\",\n",
    "    \"follows.symbol\",\n",
    "    \"subject.t1\", \"predicate.t1\", \"object.t1\", \n",
    "    \"cud.action\",\n",
    "    \"V11\", \"V12\", \"V13\", \"V14\"\n",
    "]\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4212a3c-49c7-43d9-af06-2e4e1a7c0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beffe3ec-4552-4aa7-b347-b9912ee83337",
   "metadata": {},
   "source": [
    "# Extract Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8528f8a9-b934-438d-b1ba-fe32d41e3a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract revision_id and property_id\n",
    "df['revision_id'] = df['revision.id.url'].str.extract(r\".*/(\\d+)>\")[0]\n",
    "df['property_id'] = df['predicate.t0'].str.extract(r\".*/(P\\d+)>\")[0]\n",
    "\n",
    "# Create the result_string and initialize Full_User_URL as empty\n",
    "df['result_string'] = (\n",
    "    \"https://www.wikidata.org/w/index.php?title=Property:\" + \n",
    "    df['property_id'] + \"&oldid=\" + df['revision_id']\n",
    ")\n",
    "df['Full_User_URL'] = ''  # Initialize with empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0a2c2-345a-4c6a-b858-8403ea446d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns: new columns first, followed by the rest\n",
    "new_order = ['revision_id', 'property_id', 'result_string', 'Full_User_URL'] + df.columns[:-4].tolist()\n",
    "df = df[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eefcd93-8ec2-4a71-b4d9-6d047282eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72047f86-28aa-48ae-bb5b-f586c59c2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Wikidata API base URL\n",
    "wikidata_api_url = \"https://www.wikidata.org/w/api.php\"\n",
    "output_file = \"users_one_of.tsv\"\n",
    "\n",
    "# Process each row and update the DataFrame\n",
    "for i in tqdm(range(len(df)), desc=\"Processing rows\"):\n",
    "    if pd.isna(df.loc[i, 'Full_User_URL']) or df.loc[i, 'Full_User_URL'] == '':\n",
    "        full_user_url = \"\"\n",
    "        revision_id = df.loc[i, 'revision_id']\n",
    "        \n",
    "        # API parameters\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'prop': 'revisions',\n",
    "            'revids': revision_id,\n",
    "            'rvprop': 'user|timestamp',\n",
    "            'format': 'json'\n",
    "        }\n",
    "        \n",
    "        # Make an HTTP GET request to the API\n",
    "        response = requests.get(wikidata_api_url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            json_data = response.json()\n",
    "            \n",
    "            # Navigate through the JSON response to find the user\n",
    "            pages = json_data.get('query', {}).get('pages', {})\n",
    "            for page_id, page_data in pages.items():\n",
    "                revisions = page_data.get('revisions', [])\n",
    "                if revisions:\n",
    "                    user = revisions[0].get('user')\n",
    "                    if user:\n",
    "                        # Construct full user URL\n",
    "                        full_user_url = f\"https://www.wikidata.org/wiki/User:{user}\"\n",
    "                        \n",
    "                        # Update the DataFrame with the Full_User_URL\n",
    "                        df.at[i, 'Full_User_URL'] = full_user_url\n",
    "\n",
    "        # Simulate delay to avoid overwhelming the server\n",
    "        #if (i+1) % 1000 == 0:\n",
    "        #    time.sleep(max(1, np.random.normal(3, 1)))\n",
    "\n",
    "        # Save progress to output file periodically or after each iteration\n",
    "        if (i+1) % 1000000 == 0:\n",
    "            print(\"saving backup\")\n",
    "            df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "# Final save of the complete DataFrame to the output file\n",
    "df.to_csv(output_file, sep='\\t', index=False)\n",
    "print(\"Processing complete. Output saved to:\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dcf0ec-5fa3-4e39-8444-a5d1b317fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_user_urls(df, column_name='Full_User_URL', top_n=20):\n",
    "    \"\"\"\n",
    "    Analyzes the 'Full_User_URL' column to find the top users and their share.\n",
    "    \"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Error: Column '{column_name}' not found in DataFrame.\")\n",
    "        return None\n",
    "\n",
    "    user_counts = df[column_name].value_counts()\n",
    "    total_urls = len(df[column_name])\n",
    "\n",
    "    top_users = user_counts.head(top_n)\n",
    "    top_users_df = pd.DataFrame({'Count': top_users})\n",
    "    top_users_df['Share'] = top_users_df['Count'] / total_urls * 100\n",
    "\n",
    "    return top_users_df\n",
    "\n",
    "def print_top_users(df_analysis):\n",
    "    if df_analysis is None:\n",
    "        return\n",
    "    print(\"Top Users Analysis:\")\n",
    "    for user, row in df_analysis.iterrows():\n",
    "        print(f\"User: {user}, Count: {row['Count']}, Share: {row['Share']:.2f}%\")\n",
    "\n",
    "# Example usage (assuming 'df' is your DataFrame):\n",
    "# df = pd.read_csv('your_data.csv') #if reading from a file.\n",
    "\n",
    "top_users_analysis = analyze_user_urls(df) #run the analysis, and save the result.\n",
    "print_top_users(top_users_analysis) #pass the result of the analysis to the print function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829932b",
   "metadata": {},
   "source": [
    "# Get Abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca37cb6",
   "metadata": {},
   "source": [
    "### Fetching and Processing DBPedia Abstracts\n",
    "\n",
    "This code cell implements a pipeline to retrieve abstracts from DBPedia for a list of Wikidata entities.\n",
    "\n",
    "1.  **File Reading (`read_file`, `extract_subject_t0`)**: It starts by reading a TSV or CSV file, extracting unique Wikidata QIDs from the 'subject.t0' column.\n",
    "\n",
    "2.  **Title Mapping (`get_titles`)**: It uses a local `sitelinks.en.tsv` file to map these Wikidata QIDs to English Wikipedia page titles, which are needed to query DBPedia.\n",
    "\n",
    "3.  **Abstract Fetching (`get_abstracts`)**: For each entity with a found title, it queries the DBPedia SPARQL endpoint to fetch the English abstract.\n",
    "\n",
    "4.  **Processing and Saving (`process_abstracts`)**: The main function iterates through the entities, fetches abstracts, and saves the results into separate CSV files: one for successfully retrieved abstracts, one for errors during fetching, and one for entities where no sitelink or abstract could be found. The results are processed and saved in batches. \n",
    "    - The errors file is intended to be reprocessed until no errors remain or the maximum number of retries (3) is reached.\n",
    "\n",
    "    - The missing abstracts file, containing entities without sitelinks or DBPedia abstracts, is saved for a fallback method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bdd904",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm\n",
    "%pip install pandas\n",
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12538948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import urllib.parse\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Generic function to read CSV or TSV files \n",
    "def read_file(input_file):\n",
    "    if input_file.endswith('.tsv'):\n",
    "        return pd.read_csv(input_file, sep='\\t')\n",
    "    elif input_file.endswith('.csv'):\n",
    "        return pd.read_csv(input_file)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use .csv or .tsv\")\n",
    "\n",
    "# Extract unique QIDs (subjects) from input file\n",
    "def extract_subjects(input_file):\n",
    "    df = read_file(input_file)\n",
    "    unique_subjects = df['subject.t0'].unique()\n",
    "    qids = []\n",
    "    for qid in unique_subjects:\n",
    "        cleaned_qid = str(qid).replace('<', '').replace('>', '').replace('http://www.wikidata.org/entity/', '') # Cleans up the URIs to keep only the QID (e.g., Q42).\n",
    "        qids.append(cleaned_qid)\n",
    "    print(f\"Number of unique subjects: {len(qids)}\")\n",
    "    return qids\n",
    "\n",
    "# Query the DBpedia SPARQL endpoint for an abstract\n",
    "def get_dbpedia_abstract(title):\n",
    "    endpoint = \"http://dbpedia.org/sparql\"\n",
    "    query = f\"\"\"\n",
    "    SELECT ?abstract WHERE {{\n",
    "        <http://dbpedia.org/resource/{title}> dbo:abstract ?abstract .\n",
    "        FILTER (lang(?abstract) = \"en\")\n",
    "    }}\n",
    "    \"\"\"\n",
    "    params = {'query': query, 'format': 'json'}\n",
    "\n",
    "    retries = 3\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            response = requests.get(endpoint, params=params, timeout=20)\n",
    "            if response.status_code == 200:\n",
    "                results = response.json()\n",
    "                bindings = results.get('results', {}).get('bindings', [])\n",
    "                if bindings:\n",
    "                    abstract = bindings[0]['abstract']['value']\n",
    "                    return abstract.replace('\\n', ' ').replace('\\r', ' ')\n",
    "                else:\n",
    "                    return 'No abstract found'\n",
    "            else:\n",
    "                return f\"HTTP error {response.status_code}\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Request error: {e}\"\n",
    "\n",
    "    return \"Failed after retries\"\n",
    "\n",
    "# Map QIDs to their Wikipedia titles from a sitelinks file\n",
    "def load_titles(subjects, sitelinks_file):\n",
    "    sitelinks_df = pd.read_csv(sitelinks_file, sep='\\t')\n",
    "    sitelinks_map = {}\n",
    "    for _, row in sitelinks_df.iterrows():\n",
    "        if row['label'] == 'wikipedia_sitelink':\n",
    "            qid = row['node1'].replace('<', '').replace('>', '').replace('http://www.wikidata.org/entity/', '')\n",
    "            raw_title = row['node2'].split('/')[-1]\n",
    "            # Escape special characters for a valid DBpedia resource URI\n",
    "            safe_title = urllib.parse.quote(raw_title, safe=\"()_'\")\n",
    "            sitelinks_map[qid] = safe_title\n",
    "\n",
    "    titles = {}\n",
    "    for subject in subjects:\n",
    "        if subject in sitelinks_map:\n",
    "            titles[subject] = sitelinks_map[subject]\n",
    "\n",
    "    return titles\n",
    "\n",
    "# Main loop for processing and saving abstracts \n",
    "def process_abstracts(input_file, output_file, error_file, missing_file, sitelinks_file, batch_size=1000):\n",
    "    subjects = extract_subjects(input_file)\n",
    "    titles = load_titles(subjects, sitelinks_file)\n",
    "\n",
    "    # Check if the output file exists and collect already processed QIDs\n",
    "    if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n",
    "        try:\n",
    "            print(f\"Reading existing output file: {output_file}\")\n",
    "            df_existing = pd.read_csv(output_file, usecols=['subject.t0'])\n",
    "            existing_qids = set(\n",
    "                str(qid).replace('<','').replace('>','').replace('http://www.wikidata.org/entity/','')\n",
    "                for qid in df_existing['subject.t0'].unique()\n",
    "            )\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Output file exists but is empty or corrupted: {output_file}\")\n",
    "            existing_qids = set()\n",
    "    else:\n",
    "        existing_qids = set()\n",
    "\n",
    "    batch = []\n",
    "    error_batch = []\n",
    "    missing_batch = []\n",
    "    processed_this_round = set()\n",
    "\n",
    "    # Process each QID with tqdm progress bar\n",
    "    for i, subject in enumerate(tqdm(subjects, desc=\"Processing QIDs\"), 1):\n",
    "        if subject in existing_qids:\n",
    "            continue\n",
    "\n",
    "        if subject not in titles:\n",
    "            # Log subjects with no sitelink match\n",
    "            if subject not in processed_this_round:\n",
    "                missing_batch.append([subject, 'No sitelink found'])\n",
    "                processed_this_round.add(subject)\n",
    "            continue\n",
    "\n",
    "        title = titles[subject]\n",
    "        abstract = get_dbpedia_abstract(title)\n",
    "\n",
    "        if any(keyword in abstract for keyword in ['HTTP error', 'Request error', 'Failed after retries']):\n",
    "            error_batch.append([subject, abstract])\n",
    "        elif abstract == 'No abstract found':\n",
    "            if subject not in processed_this_round:\n",
    "                missing_batch.append([subject, 'No abstract found'])\n",
    "        else:\n",
    "            batch.append([subject, abstract])\n",
    "\n",
    "        processed_this_round.add(subject)\n",
    "\n",
    "        # Write batches to disk periodically or at the end\n",
    "        if i % batch_size == 0 or i == len(subjects):\n",
    "            if batch:\n",
    "                df = pd.DataFrame(batch, columns=['subject.t0', 'abstract']).drop_duplicates(subset=['subject.t0'])\n",
    "                write_header = not os.path.exists(output_file) or os.path.getsize(output_file) == 0\n",
    "                df.to_csv(output_file, mode='a', header=write_header, index=False)\n",
    "                batch = []\n",
    "\n",
    "            if error_batch:\n",
    "                df_err = pd.DataFrame(error_batch, columns=['subject.t0', 'Error']).drop_duplicates(subset=['subject.t0'])\n",
    "                write_header = not os.path.exists(error_file) or os.path.getsize(error_file) == 0\n",
    "                df_err.to_csv(error_file, mode='a', header=write_header, index=False)\n",
    "                error_batch = []\n",
    "\n",
    "            if missing_batch:\n",
    "                df_miss = pd.DataFrame(missing_batch, columns=['subject.t0', 'Reason']).drop_duplicates(subset=['subject.t0'])\n",
    "                write_header = not os.path.exists(missing_file) or os.path.getsize(missing_file) == 0\n",
    "                df_miss.to_csv(missing_file, mode='a', header=write_header, index=False)\n",
    "                missing_batch = []\n",
    "\n",
    "    existing_qids.update(processed_this_round)\n",
    "\n",
    "# Loop to retry failed abstracts up to N iterations. After max retries, remaining failures are moved to the 'missing' file.\n",
    "def reprocess_until_done(initial_input_file, output_file, error_file, missing_file, sitelinks_file, max_iterations=3):\n",
    "    current_input = initial_input_file\n",
    "\n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        print(f\"\\n=== Iteration {iteration} ===\")\n",
    "        process_abstracts(current_input, output_file, error_file, missing_file, sitelinks_file)\n",
    "\n",
    "        if os.path.exists(error_file):\n",
    "            try:\n",
    "                errors_df = pd.read_csv(error_file)\n",
    "                if errors_df.empty:\n",
    "                    print(\"No remaining errors. Stopping loop.\")\n",
    "                    break\n",
    "                else:\n",
    "                    if iteration == max_iterations:\n",
    "                        print(f\"Max iterations reached. Moving remaining errors to missing.\")\n",
    "                        errors_df['Reason'] = 'Failed after retries'\n",
    "                        missing_header = not os.path.exists(missing_file) or os.path.getsize(missing_file) == 0\n",
    "                        errors_df[['subject.t0', 'Reason']].drop_duplicates(subset=['subject.t0']).to_csv(\n",
    "                            missing_file, mode='a', header=missing_header, index=False\n",
    "                        )\n",
    "                        print(f\"Moved {len(errors_df)} errors to missing: {missing_file}\")\n",
    "                        os.remove(error_file)\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Remaining errors: {len(errors_df)}. Retrying...\")\n",
    "                        next_input = f\"resources\\data_augmentation\\dbpedia_abstracts\\errors_iteration_{iteration}.csv\"\n",
    "                        errors_df[['subject.t0']].drop_duplicates().to_csv(next_input, index=False)\n",
    "                        current_input = next_input\n",
    "                        os.remove(error_file)\n",
    "                        \n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(\"Error file empty. Stopping loop.\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"No error file generated. Stopping loop.\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Max iterations ({max_iterations}) reached. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4564de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_input_file = 'resources/users_one_of.tsv' # Path to the initial input file with QIDs to process\n",
    "abstracts_file = 'resources/data_augmentation/dbpedia_abstracts/abstracts_oneOf.csv' # Output file where successful abstracts will be stored\n",
    "error_file = 'resources/data_augmentation/dbpedia_abstracts/errors_abstracts_oneOf.csv' # File to store temporary errors (e.g., HTTP errors, request timeouts)\n",
    "missing_file = 'resources/data_augmentation/dbpedia_abstracts/missing_abstracts_oneOf.csv' # File to store QIDs with missing abstracts or failed after retries\n",
    "sitelinks_file = 'resources/data_augmentation/sitelinks.en.tsv' # Sitelinks file mapping QIDs to Wikipedia/DBpedia titles\n",
    "\n",
    "# This will attempt to query abstracts for each QID, save errors and missing cases, and retry failed ones up to 3 times.\n",
    "# After the final iteration, remaining errors are moved to the missing file.\n",
    "reprocess_until_done(\n",
    "    initial_input_file,\n",
    "    abstracts_file,\n",
    "    error_file,\n",
    "    missing_file,\n",
    "    sitelinks_file,\n",
    "    max_iterations=3 # Number of retry rounds before giving up\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7390a0c",
   "metadata": {},
   "source": [
    "### Short abstracts\n",
    "\n",
    "This cell processes a CSV file containing abstracts. It performs the following steps:\n",
    "1.  Loads the CSV file into a pandas DataFrame.\n",
    "\n",
    "2.  Defines a function `get_first_5_sentences` that takes an abstract text, tokenizes it into sentences using NLTK, and returns only the first 5 sentences.\n",
    "\n",
    "3.  Applies the `get_first_5_sentences` function to the 'abstract' column of the DataFrame, effectively shortening each abstract.\n",
    "\n",
    "4.  Selects only the 'QID' and the processed 'abstract' columns from the DataFrame.\n",
    "\n",
    "5.  Saves the resulting DataFrame to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04812373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download the Punkt tokenizer models for sentence splitting\n",
    "\n",
    "def get_first_5_sentences(abstract):\n",
    "    # Replace line breaks with spaces to treat abstract as a single paragraph\n",
    "    abstract = ' '.join(abstract.splitlines())\n",
    "    # Tokenize the abstract into sentences\n",
    "    sentences = nltk.sent_tokenize(abstract)\n",
    "    # Join the first 5 sentences into a single string\n",
    "    first_5 = ' '.join(sentences[:5]).strip()\n",
    "    return first_5\n",
    "\n",
    "def process_abstracts(input_file, output_file):\n",
    "    # Read the input CSV file\n",
    "    df = pd.read_csv(input_file, sep=',')\n",
    "    # Apply the function to extract the first 5 sentences of each abstract\n",
    "    df['abstract'] = df['abstract'].apply(get_first_5_sentences)\n",
    "    # Save the subject and shortened abstract columns to a new CSV file\n",
    "    df[['QID', 'abstract']].to_csv(output_file, index=False)\n",
    "\n",
    "input_file = abstracts_file \n",
    "short_abstracts_file = 'resources/data_augmentation/dbpedia_abstracts/oneOf_abstracts_first_5_sentences.csv'  # Output file name\n",
    "\n",
    "process_abstracts(input_file, short_abstracts_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d33916",
   "metadata": {},
   "source": [
    "# Wikidata Text Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12de151b",
   "metadata": {},
   "source": [
    "### Collecting Wikidata Triples for Entities Missing Abstracts in DBpedia\n",
    "\n",
    "The following cells collects **all triples** from **Wikidata** for each entity (QID) that **does not have an abstract available in DBpedia**.\n",
    "\n",
    "- The list of QIDs missing abstracts has been previously identified.\n",
    "- The extracted triples will be used as input to generate informative texts about these entities.\n",
    "- These texts will be generated by a language model (LLM), which will use the triples to create detailed and useful descriptions for each entity.\n",
    "- The goal is to fill the gap of missing abstracts in DBpedia with content generated from Wikidata data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dec997",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# CONFIGURATION\n",
    "INPUT_QIDS_FILE = \"resources/data_augmentation/dbpedia_abstracts/missing_abstracts_oneOf.csv\" # Input CSV file containing QIDs (Wikidata entity IDs) to query\n",
    "OUTPUT_TRIPLES_FILE = \"resources/data_augmentation/wikidata_texts/wikidata_triples_oneOf.csv\" # Output CSV file where all collected triples will be saved\n",
    "ERRORS_FILE = \"resources/data_augmentation/wikidata_texts/query_errors_oneOf.csv\" # CSV file to save QIDs which caused query errors (to retry or analyze)\n",
    "DELETED_FILE = \"resources/data_augmentation/wikidata_texts/wikidata_deleted_entities_oneOf.csv\" # CSV file to store QIDs detected as deleted entities (no data and no redirects)\n",
    "REDIRECTS_FILE = \"resources/data_augmentation/wikidata_texts/wikidata_redirects_oneOf.csv\"  # CSV file to store redirect mappings (QIDs that redirect to other QIDs)\n",
    "\n",
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "\"\"\"\n",
    "This function takes a QID and performs a SPARQL query to fetch triples where the QID is the subject. It excludes properties of type ExternalId and CommonsMedia,as these are less \n",
    "relevant textual data. It returns a list of dictionaries with property labels, value labels (or raw values), descriptions, and the entity's label and description.\n",
    "\"\"\"\n",
    "def get_triples_for_qid(qid):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?propertyLabel \n",
    "           (IF(BOUND(?valueLabel), ?valueLabel, STR(?value)) AS ?valueDisplay)\n",
    "           (IF(BOUND(?valueDescription), ?valueDescription, \"\") AS ?valueDescription)\n",
    "           ?entityLabel \n",
    "           ?entityDescription WHERE {{\n",
    "      wd:{qid} ?statement ?value .\n",
    "      ?property wikibase:directClaim ?statement .\n",
    "      ?property wikibase:propertyType ?ptype .\n",
    "\n",
    "      FILTER(?ptype != wikibase:ExternalId && ?ptype != wikibase:CommonsMedia)\n",
    "\n",
    "      BIND(wd:{qid} AS ?entity)\n",
    "\n",
    "      OPTIONAL {{\n",
    "        FILTER(isIRI(?value))\n",
    "        ?value rdfs:label ?valueLabel .\n",
    "        ?value schema:description ?valueDescription .\n",
    "        FILTER(LANG(?valueLabel) = \"en\")\n",
    "        FILTER(LANG(?valueDescription) = \"en\")\n",
    "      }}\n",
    "\n",
    "      SERVICE wikibase:label {{\n",
    "        bd:serviceParam wikibase:language \"en\" .\n",
    "        ?property rdfs:label ?propertyLabel .\n",
    "        ?entity rdfs:label ?entityLabel .\n",
    "        ?entity schema:description ?entityDescription .\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        triples = []\n",
    "        for b in results['results']['bindings']:\n",
    "            triples.append({\n",
    "                \"QID\": qid,\n",
    "                \"Property\": b['propertyLabel']['value'],\n",
    "                \"Value\": b['valueDisplay']['value'],\n",
    "                \"ValueDescription\": b.get('valueDescription', {}).get('value', \"\"),\n",
    "                \"EntityLabel\": b['entityLabel']['value'],\n",
    "                \"EntityDescription\": b.get('entityDescription', {}).get('value', \"\")\n",
    "            })\n",
    "        return triples\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to query {qid}: {e}\")\n",
    "        return None\n",
    "\n",
    "\"\"\"\n",
    "This function checks if the input QID redirects to any other entities. Redirects in Wikidata can be modeled as owl:sameAs links.\n",
    "It returns a list of QIDs that the input QID redirects to.\n",
    "\"\"\"\n",
    "def get_redirects(qid):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?redirect WHERE {{\n",
    "      wd:{qid} owl:sameAs ?redirect.\n",
    "    }}\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        redirects = [b['redirect']['value'].split('/')[-1] for b in results['results']['bindings']]\n",
    "        return redirects\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to fetch redirects for {qid}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# This function reads QIDs from the input file, queries their triples, detects redirects and deletions, and writes the results to output files.\n",
    "def process_all_qids(\n",
    "    input_file,\n",
    "    output_file,\n",
    "    errors_file,\n",
    "    deleted_file,\n",
    "    redirects_file,\n",
    "    max_iterations=3\n",
    "):\n",
    "    # Load QIDs\n",
    "    qids_df = pd.read_csv(input_file)\n",
    "    qids = qids_df['subject.t0'].dropna().unique().tolist()\n",
    "\n",
    "    seen_triples = set() # To avoid duplicate triples\n",
    "    all_triples = [] # Accumulates all fetched triples\n",
    "    deleted = [] # QIDs with no triples and no redirects (likely deleted)\n",
    "    redirects_list = [] # Records which QIDs redirect to which others\n",
    "    errors = qids.copy() # Initially, all QIDs need processing\n",
    "    iteration = 1\n",
    "\n",
    "    # Repeat up to max_iterations or until no errors remain\n",
    "    while iteration <= max_iterations and errors:\n",
    "        print(f\"\\nIteration {iteration}/{max_iterations} | QIDs to process: {len(errors)}\")\n",
    "        next_errors = []\n",
    "        next_deleted = []\n",
    "        redirects_map = {}\n",
    "\n",
    "        for qid in tqdm(errors, desc=f\"Iteration {iteration}\"):\n",
    "            triples = get_triples_for_qid(qid)\n",
    "            if triples is None:\n",
    "                # Network error, will retry in next round\n",
    "                next_errors.append(qid)\n",
    "\n",
    "            elif len(triples) == 0:\n",
    "                # No triples found, check for redirects or deletion\n",
    "                redirs = get_redirects(qid)\n",
    "                if redirs:\n",
    "                    # Store redirects for follow-up querying\n",
    "                    redirects_map[qid] = redirs\n",
    "                else:\n",
    "                    # No redirects either; mark as deleted\n",
    "                    next_deleted.append(qid)\n",
    "            else:\n",
    "                # Save unique triples\n",
    "                for t in triples:\n",
    "                    triple_key = (\n",
    "                        t['QID'],\n",
    "                        t.get('EntityLabel', ''),\n",
    "                        t.get('EntityDescription', ''),\n",
    "                        t['Property'],\n",
    "                        t['Value'],\n",
    "                        t.get('ValueDescription', '')\n",
    "                    )\n",
    "                    if triple_key not in seen_triples:\n",
    "                        seen_triples.add(triple_key)\n",
    "                        all_triples.append(t)\n",
    "            time.sleep(1.5)  \n",
    "\n",
    "        # Process found redirects in this iteration\n",
    "        for source_qid, redirect_qids in redirects_map.items():\n",
    "            for redirected_qid in redirect_qids:\n",
    "                triples = get_triples_for_qid(redirected_qid)\n",
    "                if triples:\n",
    "                    for t in triples:\n",
    "                        triple_key = (\n",
    "                            t['QID'],\n",
    "                            t.get('EntityLabel', ''),\n",
    "                            t.get('EntityDescription', ''),\n",
    "                            t['Property'],\n",
    "                            t['Value'],\n",
    "                            t.get('ValueDescription', '')\n",
    "                        )\n",
    "                        if triple_key not in seen_triples:\n",
    "                            seen_triples.add(triple_key)\n",
    "                            all_triples.append(t)\n",
    "                else:\n",
    "                    next_errors.append(redirected_qid) # If query fails on redirected QID, mark for retry\n",
    "                time.sleep(1.5)\n",
    "                # Save redirect relation for later reference\n",
    "                redirects_list.append({\n",
    "                    \"Original_QID\": source_qid,\n",
    "                    \"Redirect_QID\": redirected_qid\n",
    "                })\n",
    "\n",
    "        # Update lists for next iteration or final save\n",
    "        deleted.extend(next_deleted)\n",
    "        errors = next_errors\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    # Save results to files\n",
    "    pd.DataFrame(all_triples).to_csv(output_file, index=False)\n",
    "    pd.DataFrame({\"QID\": errors}).to_csv(errors_file, index=False)\n",
    "    pd.DataFrame({\"QID\": deleted}).to_csv(deleted_file, index=False)\n",
    "    pd.DataFrame(redirects_list).to_csv(redirects_file, index=False)\n",
    "\n",
    "    print(\"\\nDONE!\")\n",
    "    print(f\"Total triples saved: {len(all_triples)}\")\n",
    "    print(f\"Total redirects saved: {len(redirects_list)}\")\n",
    "    print(f\"Remaining errors: {len(errors)}\")\n",
    "    print(f\"Total deleted entities: {len(deleted)}\")\n",
    "\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_qids(\n",
    "        INPUT_QIDS_FILE,\n",
    "        OUTPUT_TRIPLES_FILE,\n",
    "        ERRORS_FILE,\n",
    "        DELETED_FILE,\n",
    "        REDIRECTS_FILE,\n",
    "        max_iterations=3  # Max reprocessing rounds\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34373047",
   "metadata": {},
   "source": [
    "### Grouping by Instance to Identify Common Properties\n",
    "\n",
    "To improve the consistency and completeness of textual descriptions, we analyzed how different properties are distributed across items that share the same type (instance). The goal is to identify which properties appear most frequently within each instance group (e.g., humans, organizations, places).\n",
    "\n",
    "By grouping items by their \"instance of\" value and counting how often each property appears within those groups, we can discover patterns and define a common structure for textual summaries. This can help ensure that similar types of entities are described in a coherent and standardized way across the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f013e7",
   "metadata": {},
   "source": [
    "#### 1. Building the property ranking per instance\n",
    "\n",
    "- Loads the original triples file (`wikidata_triples_oneOf.csv`).\n",
    "- Identifies the instance(s) of each QID using the `instance of` property.\n",
    "- Assigns a default tag `without instance of` (no instance) if none found.\n",
    "- Groups data by instance and property, counting how many unique QIDs have each property.\n",
    "- Creates a ranking of properties for each instance, ordered by frequency (`count_qids`).\n",
    "- Saves this ranking to `property_ranking_by_instance.csv`.\n",
    "\n",
    "**Purpose:** to understand which properties are most frequent for each entity type (instance).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b61e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     instance_of                            predicate  count_qids\n",
      "4891       human                          instance of        4612\n",
      "4994       human                        sex or gender        4608\n",
      "4939       human                           occupation        4159\n",
      "4832       human                        date of birth        4155\n",
      "4823       human               country of citizenship        3872\n",
      "4872       human                           given name        3848\n",
      "4959       human                       place of birth        3462\n",
      "4898       human  languages spoken, written or signed        2659\n",
      "4861       human                          family name        2544\n",
      "4834       human                        date of death        2543\n",
      "4961       human                       place of death        1944\n",
      "4769       human                     Commons category        1300\n",
      "4850       human                          educated at        1096\n",
      "4925       human              name in native language         756\n",
      "4798       human                       award received         660\n",
      "4907       human                      manner of death         653\n",
      "4969       human                        position held         615\n",
      "4804       human                       cause of death         597\n",
      "4852       human                             employer         507\n",
      "4960       human                      place of burial         489\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"resources/data_augmentation/wikidata_texts/wikidata_triples_oneOf.csv\") # Load the CSV file containing the collected Wikidata triples\n",
    "\n",
    "# Create mapping QID → instance (for those with 'instance of' property)\n",
    "instance_map = df[df['Property Label'] == 'instance of'][['QID', 'Value Display']]\n",
    "instance_map = instance_map.rename(columns={'Value Display': 'instance_of'})\n",
    "\n",
    "df = df.merge(instance_map, on='QID', how='left') # Merge to add the instance, if any\n",
    "\n",
    "df['instance_of'] = df['instance_of'].fillna('without instance of') # Fill QIDs without instance with a default tag\n",
    "\n",
    "grouped = df.groupby(['instance_of', 'Property Label'])['QID'].nunique().reset_index() # Group by instance and property, counting unique QIDs\n",
    "\n",
    "grouped = grouped.rename(columns={'Property Label': 'predicate','QID': 'count_qids'}) # Rename columns to the desired format\n",
    "\n",
    "grouped = grouped.sort_values(by='count_qids', ascending=False) # Sort from highest to lowest count\n",
    "\n",
    "grouped.to_csv(\"resources/data_augmentation/wikidata_texts/property_ranking_by_instance.csv\", index=False) # Save to CSV\n",
    "\n",
    "print(grouped.head(20)) # Display the top 20 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5458e06",
   "metadata": {},
   "source": [
    "#### 2. Filtering triples using a combined property ranking\n",
    "\n",
    "- Loads the triples file and the property ranking per instance created earlier.\n",
    "- For each QID, retrieves its associated instances.\n",
    "- For each instance, gets the ordered list of properties by frequency ranking.\n",
    "- The frequency ranking for each instance is first normalized by assigning descending weights (e.g., top 1 gets N, next gets N-1, etc.) so that higher-ranked properties contribute more to the final score.\n",
    "- Computes a combined ranking by summing the weights across instances for each property.\n",
    "- For each QID, selects up to 15 most relevant properties according to the combined ranking that appear in the QID’s triples.\n",
    "- Removes triples whose values start with Wikidata links (to exclude less informative).\n",
    "- Saves the filtered triples to `triples_top15_by_combined_instance.csv`.\n",
    "\n",
    "**Purpose:** to select the most informative and relevant triples for each entity, considering multiple instances and property frequency, reducing noise and prioritizing important data.  \n",
    "This assumes that the most frequent properties are likely those most commonly used by the Wikidata community to describe entities of that instance type, so they tend to capture the essential aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac86f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_203846/367343574.py:11: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sort_values('count_qids', ascending=False)['predicate'].tolist())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total selected triples: 71810\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "detailed = pd.read_csv(\"resources/data_augmentation/wikidata_texts/wikidata_triples_oneOf.csv\") # Load the detailed triples CSV\n",
    "\n",
    "ranking = pd.read_csv(\"resources/data_augmentation/wikidata_texts/property_ranking_by_instance.csv\") # Load the property ranking per instance CSV\n",
    "\n",
    "# Create dictionary: instance -> list of properties ordered by frequency\n",
    "top_props_per_instance = (\n",
    "    ranking.groupby('instance_of')\n",
    "           .apply(lambda g: g.sort_values('count_qids', ascending=False)['predicate'].tolist())\n",
    "           .to_dict()\n",
    ")\n",
    "\n",
    "# Map instances of each QID (may have multiple instances)\n",
    "instances_per_qid = (\n",
    "    detailed[detailed['Property Label'] == 'instance of']\n",
    "    .groupby('QID')['Value Display'].apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Normalized combined ranking summing weighted points\n",
    "def combined_ranking_normalized(instances):\n",
    "    if not instances:\n",
    "        return top_props_per_instance.get('without instance of', [])\n",
    "\n",
    "    counter = Counter()\n",
    "\n",
    "    for inst in instances:\n",
    "        props = top_props_per_instance.get(inst, [])\n",
    "        total_props = len(props)\n",
    "        if total_props == 0:\n",
    "            continue\n",
    "        \n",
    "        for i, prop in enumerate(props):\n",
    "            weight = total_props - i # Original weight: descending by position\n",
    "            normalized_weight = weight / total_props # Normalize by dividing by the total number of properties in the instance\n",
    "            counter[prop] += normalized_weight\n",
    "\n",
    "    # Sort properties from highest combined normalized weight to lowest\n",
    "    final_ranking = [prop for prop, _ in counter.most_common()]\n",
    "    return final_ranking\n",
    "\n",
    "results = []\n",
    "\n",
    "# Process each QID individually\n",
    "for qid, group in detailed.groupby('QID'):\n",
    "    if(qid.startswith('Q')):\n",
    "        instances = instances_per_qid.get(qid, [])\n",
    "        ranking_for_item = combined_ranking_normalized(instances)\n",
    "        \n",
    "        available_props = group['Property Label'].tolist()\n",
    "        \n",
    "        top_props = [p for p in ranking_for_item if p in available_props][:15] # Take up to 15 properties following the combined ranking that exist in the item\n",
    "        \n",
    "        filtered_group = group[group['Property Label'].isin(top_props)]\n",
    "        results.append(filtered_group)\n",
    "\n",
    "final = pd.concat(results) # Concatenate all filtered groups\n",
    "\n",
    "final = final[~final['Value Display'].fillna('').str.startswith(\"http\")] # Remove rows where 'Value Display' starts with a Wikidata link (exclude these values)\n",
    "\n",
    "# Save final CSV without the 'instance_of' column\n",
    "final.drop(columns=['instance_of'], errors='ignore').to_csv(\"resources/data_augmentation/wikidata_texts/triples_top15_by_combined_instance.csv\", index=False)\n",
    "\n",
    "print(f\"Total selected triples: {len(final)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717dba3e",
   "metadata": {},
   "source": [
    "### Generating Texts from Wikidata Triples\n",
    "\n",
    "This cell takes structured data (triples) about entities extracted from Wikidata and automatically generates informative texts for each entity. These texts are meant to replace missing abstracts from DBpedia.\n",
    "\n",
    "The process includes:\n",
    "- Grouping triples by entity (QID).\n",
    "- Sending facts, labels, and descriptions to a language model to generate a paragraph about each entity.\n",
    "- Entities producing empty texts, texts with more than 5 # symbols, or fewer than 2 sentences are marked as problematic.\n",
    "- Well-formed outputs are saved separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d91a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import time\n",
    "import subprocess\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download punkt tokenizer for sentence splitting\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Generate a text paragraph using a language model.Constructs a prompt with examples and facts, sends request via subprocess, and extracts the generated response.\n",
    "def generate_paragraph(entity_label, description, triples):\n",
    "    \"\"\"    \n",
    "    Args:\n",
    "        entity_label (str): The entity's label.\n",
    "        description (str): The entity's description.\n",
    "        triples (list of tuples): List of (predicate, object(the object may include a description when available.)) facts about the entity.\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated paragraph\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format facts as bullet points\n",
    "    formatted_facts = \"\\n\".join([f\"- {p}: {o} - {d}\" for p, o, d in triples])\n",
    "    \n",
    "    # Prompt including examples and facts to guide the LLM\n",
    "    prompt = f\"\"\"Below are two examples of paragraphs written in a formal, encyclopedic style. Use them as a reference for writing a new text.\n",
    "\n",
    "    Example 1:\n",
    "    \"NGC 6212 is a spiral galaxy located in the constellation Hercules. It is designated as Sb in the galaxy morphological classification scheme and was discovered by the French astronomer Édouard Stephan on 26 July 1870. NGC 6212 is located at about 397 million light years from Earth.\"\n",
    "\n",
    "    Example 2:\n",
    "    \"Franz Josef Heinz, known as Heinz-Orbis, (25 February 1884 - 9 January 1924) was a Palatine separatist who briefly led the government of the \\\"Autonomous Palatinate\\\" during the French occupation of the Rhineland. He was assassinated by German nationalists in 1924. Heinz came from the town of Orbis in Northern Palatinate, later using the town as part of his name. He was a farmer and became a leader of the free peasantry and the founder of the Palatine Corps. In the aftermath of World War I, France occupied the Rhineland.\"\n",
    "\n",
    "    Using the following facts, write a similar paragraph about ({entity_label} - {description}) in formal English:\n",
    "    IMPORTANT:\n",
    "    - Do NOT include any property IDs (like P21, P27) or QIDs (like Q581).\n",
    "    - Always convert any identifiers into human-readable labels and context.\n",
    "    - Write naturally and smoothly, in full sentences and do not add anything else\n",
    "\n",
    "    {formatted_facts}\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"mixtral:8x7b\",\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [\n",
    "            \"curl\", \"-s\", \"-u\", \"llama:miengohNg9OG6ieR5aof\",\n",
    "            \"-H\", \"Content-Type: application/json\",\n",
    "            \"-X\", \"POST\", \"https://llama-webui.ai.wu.ac.at/api/generate\",\n",
    "            \"-d\", json.dumps(data)\n",
    "        ],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "    )\n",
    "\n",
    "    generated_response = \"\"\n",
    "    # Process each line of output, expecting JSON lines with a \"response\" field\n",
    "    for line in result.stdout.strip().splitlines():\n",
    "        try:\n",
    "            item = json.loads(line)\n",
    "            generated_response += item.get(\"response\", \"\")\n",
    "        except json.JSONDecodeError:\n",
    "            continue # Ignore lines that are not valid JSON\n",
    "\n",
    "    return generated_response.strip()\n",
    "\n",
    "# Reads input CSV of triples, groups data by QID, generates texts with the LLM, and writes results and errors to output CSV files.\n",
    "def process_csv_and_generate_texts(input_path, results_file, errors_file):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_path (str): Path to the CSV input file containing the triples.\n",
    "        results_file (str): Path to the output CSV file for successful generations.\n",
    "        errors_file (str): Path to the output CSV file for any generation errors.\n",
    "    \"\"\"\n",
    "\n",
    "    entities = {}\n",
    "\n",
    "    # Read triples from input CSV and group by QID\n",
    "    with open(input_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            qid = row[\"QID\"]\n",
    "            if qid not in entities:\n",
    "                entities[qid] = {\n",
    "                    \"Entity Label\": row[\"Entity Label\"],\n",
    "                    \"Entity Description\": row[\"Entity Description\"],\n",
    "                    \"triples\": []\n",
    "                }\n",
    "            entities[qid][\"triples\"].append((row[\"Property Label\"], row[\"Value Display\"], row[\"Value Description\"]))\n",
    "\n",
    "    with open(results_file, \"a\", encoding=\"utf-8\", newline='') as out_csv, \\\n",
    "         open(errors_file, \"w\", encoding=\"utf-8\", newline='') as err_csv:\n",
    "\n",
    "        writer_out = csv.writer(out_csv)\n",
    "        writer_err = csv.writer(err_csv)\n",
    "\n",
    "        writer_out.writerow([\"QID\", \"text\"])\n",
    "        writer_err.writerow([\"QID\", \"error\"])\n",
    "\n",
    "        for qid, data in tqdm(entities.items(), desc=\"Generating texts\"):\n",
    "            try:\n",
    "                paragraph = generate_paragraph(data[\"Entity Label\"], data[\"Entity Description\"], data[\"triples\"])\n",
    "                paragraph_clean = paragraph.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "\n",
    "                # Check for invalid outputs and write to errors if necessary\n",
    "                if (not paragraph_clean or paragraph_clean.count(\"#\") > 5 or len(sent_tokenize(paragraph_clean)) < 2):\n",
    "                    writer_err.writerow([qid, \"Invalid or poor quality response\"])\n",
    "                else:\n",
    "                    writer_out.writerow([qid, paragraph_clean])\n",
    "\n",
    "                time.sleep(1.5)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log exceptions to error file\n",
    "                writer_err.writerow([qid, str(e)])\n",
    "    \n",
    "    print(f\"\\nProcess finished!\")\n",
    "    print(f\"- Results saved in: {results_file}\")\n",
    "    print(f\"- Errors saved in: {errors_file}\")\n",
    "\n",
    "\n",
    "# CONFIGURATION \n",
    "# Define the input CSV file with triples \n",
    "INPUT_FILE = \"resources/data_augmentation/wikidata_texts/triples_top15_by_combined_instance.csv\" \n",
    "\n",
    "# Define the output CSV filenames for the results and errors\n",
    "RESULTS_OUTPUT = \"resources/data_augmentation/wikidata_texts/wikidata_texts_oneOf.csv\"\n",
    "ERRORS_OUTPUT = \"resources/data_augmentation/wikidata_texts/texts_errors_oneOf_.csv\"\n",
    "\n",
    "# MAIN EXECUTION \n",
    "# Run the main pipeline: process the triples and generate texts, saving successful results and any errors separately\n",
    "process_csv_and_generate_texts(INPUT_FILE, RESULTS_OUTPUT, ERRORS_OUTPUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c4b82f",
   "metadata": {},
   "source": [
    "### Merge Abstracts with Wikidata Texts\n",
    "This script merges two CSV files containing entity descriptions:\n",
    "\n",
    "- `abstracts_oneOf.csv`: abstracts from DBpedia  \n",
    "- `wikidata_texts_oneOf.csv`: generated texts for entities **missing DBpedia abstracts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files successfully merged into: final_entity_texts_oneOf.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Input file paths\n",
    "file1 = 'resources/data_augmentation/dbpedia_abstracts/abstracts_oneOf.csv'\n",
    "file2 = 'resources/data_augmentation/wikidata_texts/wikidata_texts_oneOf.csv'\n",
    "output = 'resources/data_augmentation/final_entity_texts_oneOf.csv'\n",
    "\n",
    "header = [\"QID\", \"abstract\"]\n",
    "\n",
    "# Open the output file and write combined data\n",
    "with open(output, \"w\", newline='', encoding=\"utf-8\") as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow(header) \n",
    "\n",
    "    def read_rows(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader) \n",
    "            for row in reader:\n",
    "                if row:  # Skip empty lines\n",
    "                    writer.writerow(row)\n",
    "\n",
    "    # Append content from both files\n",
    "    read_rows(file1)\n",
    "    read_rows(file2)\n",
    "\n",
    "print(f\"Files successfully merged into: {output}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
